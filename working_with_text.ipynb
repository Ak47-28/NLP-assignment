{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Text Data - Preprocessing and Text to Numerical Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data\n",
    "\n",
    "Text Analysis is a major application field for machine learning algorithms. Some of the major application areas of NLP are:\n",
    "1. Spell Checker, Keyword Search, etc\n",
    "2. Sentiment Analysis, Spam Classification\n",
    "3. Machine Translation\n",
    "4. Chatbots/Dialog Systems\n",
    "5. Question Answering Systems\n",
    "etc..\n",
    "\n",
    "However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
    "\n",
    "### Why NLP is hard?\n",
    "1. Complexity of representation\n",
    "2. Ambiguity in Natural Langugae\n",
    "\n",
    "\n",
    "### Text Preprocessing\n",
    "\n",
    "1. Tokenisation  \n",
    "2. Removing special characters  \n",
    "3. Convert sentence into lower case  \n",
    "4. Removing stop words  \n",
    "5. Stemming or Lemmatization\n",
    "\n",
    "\n",
    "### Techniques to convert Text to Numerical Vectors\n",
    "\n",
    "1. Bag of Words\n",
    "2. TF IDF (Term Frequency - Inverse Document Frequency)\n",
    "3. Word2Vec (by Google)\n",
    "4. GloVe (Global Vectors by Stanford) - **Not Covered in this notebook**\n",
    "5. FastText (by Facebook) - **Not Covered in this notebook**\n",
    "6. ELMo (Embeddings from Language Models) - **Not Covered in this notebook**\n",
    "7. BERT (Bidirectional Encoder Representations from Transformer) - **Not Covered in this notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text\n",
       "0     it Was the best oF Times $\n",
       "1     It was The worst of times.\n",
       "2     IT 9 was tHe age Of wisdom\n",
       "3  it was thE age of foolishness"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_text = ['it Was the best oF Times $', \n",
    "            'It was The worst of times.',\n",
    "            'IT 9 was tHe age Of wisdom', \n",
    "            'it was thE age of foolishness']\n",
    "\n",
    "df = pd.DataFrame({'text': lst_text})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# Downloading wordnet before applying Lemmatizer\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialise the inbuilt Stemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also use Lemmatizer instead of Stemmer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing Steps\n",
    "\n",
    "Text Preprocessing steps include some essential tasks to clean and remove the noise from the available data.\n",
    "\n",
    "1. **Removing Special Characters and Punctuation**\n",
    "\n",
    "2. **Converting to Lower Case** - We convert the whole text corpus to lower case to reduce the size of the vocabulary of our text data.\n",
    "\n",
    "3. **Removing Stop Words** - Stopwords don't contribute to the meaning of a sentence. So, we can safely remove them without changing the meaning of the sentence. For eg: it, was, any, then, a, is, by, etc are the stopwords.\n",
    "\n",
    "4. **Stemming or Lemmatization** - Stemming is the process of getting the root form of a word. For eg: warm, warmer, warming can be converted to warm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This 1is Natural-LAnguage-Processing.\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"This 1is Natural-LAnguage-Processing.\"\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This  is Natural LAnguage Processing \n"
     ]
    }
   ],
   "source": [
    "# Removing special characters and digits\n",
    "sentence = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this  is natural language processing \n"
     ]
    }
   ],
   "source": [
    "# change sentence to lower case\n",
    "sentence = sentence.lower()\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'natural', 'language', 'processing']\n"
     ]
    }
   ],
   "source": [
    "# tokenize into words\n",
    "tokens = sentence.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing']\n"
     ]
    }
   ],
   "source": [
    "# Removing stop words\n",
    "clean_tokens = [t for t in tokens if not t in stopwords.words(\"english\")]\n",
    "print(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "clean_tokens_stem = [stemmer.stem(word) for word in clean_tokens]\n",
    "print(clean_tokens_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\piyus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing\n",
    "clean_tokens_lem = [lemmatizer.lemmatize(word) for word in clean_tokens]\n",
    "print(clean_tokens_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_text, flag):\n",
    "    # Removing special characters and digits\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    \n",
    "    # change sentence to lower case\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    tokens = sentence.split()\n",
    "    \n",
    "    # remove stop words                \n",
    "    clean_tokens = [t for t in tokens if not t in stopwords.words(\"english\")]\n",
    "    \n",
    "    # Stemming/Lemmatization\n",
    "    if(flag == 'stem'):\n",
    "        clean_tokens = [stemmer.stem(word) for word in clean_tokens]\n",
    "    else:\n",
    "        clean_tokens = [lemmatizer.lemmatize(word) for word in clean_tokens]\n",
    "    \n",
    "    return pd.Series([\" \".join(clean_tokens), len(clean_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0  1\n",
       "0    best time  2\n",
       "1   worst time  2\n",
       "2   age wisdom  2\n",
       "3  age foolish  2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = df['text'].apply(lambda x : preprocess(x, 'stem'))\n",
    "\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  clean_text_stem  text_length_stem\n",
       "0       best time                 2\n",
       "1      worst time                 2\n",
       "2      age wisdom                 2\n",
       "3     age foolish                 2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.columns = ['clean_text_stem', 'text_length_stem']\n",
    "\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text clean_text_stem  text_length_stem\n",
       "0     it Was the best oF Times $       best time                 2\n",
       "1     It was The worst of times.      worst time                 2\n",
       "2     IT 9 was tHe age Of wisdom      age wisdom                 2\n",
       "3  it was thE age of foolishness     age foolish                 2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df, temp_df], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0  1\n",
       "0        best time  2\n",
       "1       worst time  2\n",
       "2       age wisdom  2\n",
       "3  age foolishness  2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = df['text'].apply(lambda x: preprocess(x, 'lemma'))\n",
    "\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  clean_text_lemma  text_length_lemma\n",
       "0        best time                  2\n",
       "1       worst time                  2\n",
       "2       age wisdom                  2\n",
       "3  age foolishness                  2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.columns = ['clean_text_lemma', 'text_length_lemma']\n",
    "\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text clean_text_stem  text_length_stem  \\\n",
       "0     it Was the best oF Times $       best time                 2   \n",
       "1     It was The worst of times.      worst time                 2   \n",
       "2     IT 9 was tHe age Of wisdom      age wisdom                 2   \n",
       "3  it was thE age of foolishness     age foolish                 2   \n",
       "\n",
       "  clean_text_lemma  text_length_lemma  \n",
       "0        best time                  2  \n",
       "1       worst time                  2  \n",
       "2       age wisdom                  2  \n",
       "3  age foolishness                  2  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df, temp_df], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Word Representation\n",
    "\n",
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.\n",
    "\n",
    "We will use `CountVectorizer` to **convert text into a matrix of token count**.\n",
    "\n",
    "`Bag of Words`: https://machinelearningmastery.com/gentle-introduction-bag-words-model/\n",
    "\n",
    "`Code Example`: https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/  \n",
    "\n",
    "**We are going to perform below mentioned steps to understand the entire process:**  \n",
    "a. Converting text to numerical vectors with the help of `CountVectorizer`  \n",
    "b. Understand `fit` and `transform`  \n",
    "c. Looking at `vocabulary_`  \n",
    "d. Converting sparse matrix to dense matrix using `toarray()`  \n",
    "e. Understanding `n_gram`  \n",
    "\n",
    "### Advantages\n",
    "1. It is simple to understand and implement like OneHotEncoding.\n",
    "2. We have a fixed length encoding for any sequence of arbitrary length.\n",
    "3. Documents with same words/vocabulary will have similar representation. So if two documents have a similar vocabulary, they’ll be closer to each other in the vector space and vice versa.\n",
    "\n",
    "### Disadvantages\n",
    "1. The size of vector increases with the size of the vocabulary. Thus, sparsity continues to be a problem. One way to control it is by limiting the vocabulary to n number of the most frequent words.\n",
    "2. It does not capture the similarity between different words that mean the same thing. Say we have three documents: “walk”, “walked”, and “walking”. BoW vectors of all three documents will be equally apart.\n",
    "3. This representation does not have any way to handle out of vocabulary (OOV) words (i.e., new words that were not seen in the corpus that was used to build the vectorizer).\n",
    "4. As the name indicates, it is a “bag” of words. Word order information is lost in this representation. One way to control it is by using n-grams.\n",
    "5. It suffers from **curse of high dimensionality.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text clean_text_stem  text_length_stem  \\\n",
       "0     it Was the best oF Times $       best time                 2   \n",
       "1     It was The worst of times.      worst time                 2   \n",
       "2     IT 9 was tHe age Of wisdom      age wisdom                 2   \n",
       "3  it was thE age of foolishness     age foolish                 2   \n",
       "\n",
       "  clean_text_lemma  text_length_lemma  \n",
       "0        best time                  2  \n",
       "1       worst time                  2  \n",
       "2       age wisdom                  2  \n",
       "3  age foolishness                  2  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.\n",
    "vocab = CountVectorizer()\n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "\n",
    "dtm = vocab.fit_transform(df['clean_text_lemma'])\n",
    "\n",
    "# fit_transform() could be done seperatly as mentioned below\n",
    "# vocab.fit(df.clean_text_stem)\n",
    "# dtm = vocab.transform(df.clean_text_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best': 1, 'time': 3, 'worst': 5, 'age': 0, 'wisdom': 4, 'foolishness': 2}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can look at unique words by using 'vocabulary_'\n",
    "\n",
    "vocab.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# Observe that the type of dtm is sparse\n",
    "\n",
    "print(type(dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6)\n"
     ]
    }
   ],
   "source": [
    "# Lets now print the  shape of this dtm\n",
    "\n",
    "print(dtm.shape)\n",
    "\n",
    "# o/p -> (4, 6)\n",
    "# i.e -> 4 documents and 6 unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 2)\t1\n"
     ]
    }
   ],
   "source": [
    "# Lets look at the dtm\n",
    "\n",
    "print(dtm)\n",
    "\n",
    "# Remember that dtm is a sparse matrix. i.e. zeros wont be stored\n",
    "# Lets understand First line of output -> (0,1)    1\n",
    "# Here (0, 1) means 0th document and 1st(index starting from 0) unique word. \n",
    "# (we have total 4 documents) & (we have total 6 unique words)\n",
    "# (0, 1)    1 -> 1 here refers to the number of occurence of 1st word\n",
    "# Now lets read it all in english.\n",
    "# (0, 1)    1 -> 'times' occurs 1 time in 0th document. \n",
    "# Try to observe -> (3, 2)   1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 0]\n",
      " [0 0 0 1 0 1]\n",
      " [1 0 0 0 1 0]\n",
      " [1 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Since the dtm is sparse, lets convert it into numpy array.\n",
    "\n",
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best': 1, 'time': 3, 'worst': 5, 'age': 0, 'wisdom': 4, 'foolishness': 2}\n",
      "dict_keys(['best', 'time', 'worst', 'age', 'wisdom', 'foolishness'])\n",
      "[('age', 0), ('best', 1), ('foolishness', 2), ('time', 3), ('wisdom', 4), ('worst', 5)]\n",
      "['age', 'best', 'foolishness', 'time', 'wisdom', 'worst']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = vocab.vocabulary_\n",
    "\n",
    "print(vocabulary)\n",
    "\n",
    "print(vocabulary.keys())\n",
    "\n",
    "sort_vocab_tup = sorted(vocabulary.items(), key = lambda x : x[1])\n",
    "\n",
    "print(sort_vocab_tup)\n",
    "\n",
    "sort_vocab = [tup[0] for tup in sort_vocab_tup]\n",
    "\n",
    "print(sort_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'best', 'foolishness', 'time', 'wisdom', 'worst']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vocab.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>best</th>\n",
       "      <th>foolishness</th>\n",
       "      <th>time</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  best  foolishness  time  wisdom  worst\n",
       "0    0     1            0     1       0      0\n",
       "1    0     0            0     1       0      1\n",
       "2    1     0            0     0       1      0\n",
       "3    1     0            1     0       0      0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dtm.toarray(), columns=sorted(vocab.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-grams\n",
    "\n",
    "vocab = CountVectorizer(ngram_range=[1,2])\n",
    "\n",
    "dtm = vocab.fit_transform(df.clean_text_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best': 3, 'time': 6, 'best time': 4, 'worst': 8, 'worst time': 9, 'age': 0, 'wisdom': 7, 'age wisdom': 2, 'foolish': 5, 'age foolish': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vocab.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 1 1]\n",
      " [1 0 1 0 0 0 0 1 0 0]\n",
      " [1 1 0 0 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# convert sparse matrix to numpy array\n",
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age foolish</th>\n",
       "      <th>age wisdom</th>\n",
       "      <th>best</th>\n",
       "      <th>best time</th>\n",
       "      <th>foolish</th>\n",
       "      <th>time</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "      <th>worst time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  age foolish  age wisdom  best  best time  foolish  time  wisdom  \\\n",
       "0    0            0           0     1          1        0     1       0   \n",
       "1    0            0           0     0          0        0     1       0   \n",
       "2    1            0           1     0          0        0     0       1   \n",
       "3    1            1           0     0          0        1     0       0   \n",
       "\n",
       "   worst  worst time  \n",
       "0      0           0  \n",
       "1      1           1  \n",
       "2      0           0  \n",
       "3      0           0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dtm.toarray(), columns=sorted(vocab.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- `vect.fit(lst_text)` **learns the vocabulary**\n",
    "- `vect.transform(lst_text)` **uses the fitted vocabulary** to build a **document-term matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency Inverse Document Frequency\n",
    "\n",
    "In BOW approach all the words in the text are treated as equally important i.e. there's no notion of some words in the document being more important than others. TF-IDF, or term frequency-inverse document frequency, addresses this issue. It aims to quantify the importance of a given word relative to other words in the document and in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "dtm = vectorizer.fit_transform(df.clean_text_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best': 1, 'time': 3, 'worst': 5, 'age': 0, 'wisdom': 4, 'foolishness': 2}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.78528828 0.         0.6191303  0.         0.        ]\n",
      " [0.         0.         0.         0.6191303  0.         0.78528828]\n",
      " [0.6191303  0.         0.         0.         0.78528828 0.        ]\n",
      " [0.6191303  0.         0.78528828 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(dtm.toarray()) \n",
    "\n",
    "# convert sparse matrix to nparray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>best</th>\n",
       "      <th>foolishness</th>\n",
       "      <th>time</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.785288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.61913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.61913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.61913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.785288</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.61913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785288</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age      best  foolishness     time    wisdom     worst\n",
       "0  0.00000  0.785288     0.000000  0.61913  0.000000  0.000000\n",
       "1  0.00000  0.000000     0.000000  0.61913  0.000000  0.785288\n",
       "2  0.61913  0.000000     0.000000  0.00000  0.785288  0.000000\n",
       "3  0.61913  0.000000     0.785288  0.00000  0.000000  0.000000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dtm.toarray(), columns=sorted(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "\"You shall know the word by the company it keeps.\" by JR Firth\n",
    "\n",
    "**Distributional Semantics (i.e. a word is characterized by the company it keeps)**  \n",
    "W2v works well because there is an idea of meaning distribution in the context.\n",
    "\n",
    "**Algorithms to generate Word2Vec Embeddings**\n",
    "1. SkipGram\n",
    "2. Continuous Bag of Words\n",
    "\n",
    "**Issue**  \n",
    "Even if the word is having three different meaning, W2v will return the weighted average of all three as the output. Now the question is, \n",
    "- Is it possible to segregate the three vectors to represent the words based in the context? \n",
    "$$ OR $$\n",
    "- Is it possible to disambiguate the word vectors based on the context?\n",
    "\n",
    "Word2Vec is not capturing the contextual information. This is where BERT comes handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`! pip install gensim`**  \n",
    "Run this in command promp (admin mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "      <th>tokenised_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>[best, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>[worst, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, wisdom]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, foolish]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text clean_text_stem  text_length_stem  \\\n",
       "0     it Was the best oF Times $       best time                 2   \n",
       "1     It was The worst of times.      worst time                 2   \n",
       "2     IT 9 was tHe age Of wisdom      age wisdom                 2   \n",
       "3  it was thE age of foolishness     age foolish                 2   \n",
       "\n",
       "  clean_text_lemma  text_length_lemma tokenised_sentences  \n",
       "0        best time                  2        [best, time]  \n",
       "1       worst time                  2       [worst, time]  \n",
       "2       age wisdom                  2       [age, wisdom]  \n",
       "3  age foolishness                  2      [age, foolish]  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenised_sentences'] = df.clean_text_stem.apply(lambda sent : sent.split())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['best', 'time'], ['worst', 'time'], ['age', 'wisdom'], ['age', 'foolish']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.tokenised_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "\n",
    "model = Word2Vec(list(df.tokenised_sentences), vector_size=100, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=6, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 0, 'time': 1, 'foolish': 2, 'wisdom': 3, 'worst': 4, 'best': 5}\n",
      "['age', 'time', 'foolish', 'wisdom', 'worst', 'best']\n"
     ]
    }
   ],
   "source": [
    "# Looking at the vocabulary\n",
    "\n",
    "print(model.wv.key_to_index)\n",
    "\n",
    "print(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.4563962e-05  3.0773187e-03 -6.8126465e-03 -1.3754654e-03\n",
      "  7.6685809e-03  7.3464084e-03 -3.6732983e-03  2.6427007e-03\n",
      " -8.3171297e-03  6.2054847e-03 -4.6373224e-03 -3.1641079e-03\n",
      "  9.3113566e-03  8.7338447e-04  7.4907015e-03 -6.0740639e-03\n",
      "  5.1605059e-03  9.9228211e-03 -8.4573915e-03 -5.1356913e-03\n",
      " -7.0648384e-03 -4.8626517e-03 -3.7785650e-03 -8.5362010e-03\n",
      "  7.9556061e-03 -4.8439382e-03  8.4236125e-03  5.2625705e-03\n",
      " -6.5500261e-03  3.9578700e-03  5.4701497e-03 -7.4265362e-03\n",
      " -7.4057197e-03 -2.4752307e-03 -8.6257271e-03 -1.5815735e-03\n",
      " -4.0343284e-04  3.2996845e-03  1.4418793e-03 -8.8142155e-04\n",
      " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936899e-03\n",
      "  3.9735888e-03  4.5294715e-03  1.4343048e-03 -2.6998566e-03\n",
      " -4.3668128e-03 -1.0320758e-03  1.4370275e-03 -2.6460099e-03\n",
      " -7.0737838e-03 -7.8053069e-03 -9.1217877e-03 -5.9351707e-03\n",
      " -1.8474245e-03 -4.3238713e-03 -6.4606713e-03 -3.7173224e-03\n",
      "  4.2891572e-03 -3.7390448e-03  8.3781742e-03  1.5339922e-03\n",
      " -7.2423196e-03  9.4337985e-03  7.6312111e-03  5.4932809e-03\n",
      " -6.8488456e-03  5.8226776e-03  4.0090918e-03  5.1853680e-03\n",
      "  4.2559002e-03  1.9397545e-03 -3.1701636e-03  8.3538434e-03\n",
      "  9.6121784e-03  3.7926030e-03 -2.8369951e-03  7.1263312e-06\n",
      "  1.2188172e-03 -8.4583256e-03 -8.2239462e-03 -2.3101569e-04\n",
      "  1.2372875e-03 -5.7433820e-03 -4.7252751e-03 -7.3460746e-03\n",
      "  8.3286138e-03  1.2129784e-04 -4.5093987e-03  5.7017040e-03\n",
      "  9.1800140e-03 -4.0998720e-03  7.9646800e-03  5.3754328e-03\n",
      "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# access the 100 dimensional vector for one of the words\n",
    "\n",
    "print(model.wv.__getitem__('foolish'))\n",
    "\n",
    "print(model.wv.__getitem__('foolish').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.3622725e-04  2.3643016e-04  5.1033497e-03  9.0092728e-03\n",
      "  -9.3029495e-03 -7.1168090e-03  6.4588715e-03  8.9729885e-03\n",
      "  -5.0154282e-03 -3.7633730e-03  7.3805046e-03 -1.5334726e-03\n",
      "  -4.5366143e-03  6.5540504e-03 -4.8601604e-03 -1.8160177e-03\n",
      "   2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488189e-03\n",
      "   7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "   6.3508893e-03 -3.4053659e-03 -9.4640255e-04  5.7685734e-03\n",
      "  -7.5216386e-03 -3.9361049e-03 -7.5115822e-03 -9.3004224e-04\n",
      "   9.5381187e-03 -7.3191668e-03 -2.3337698e-03 -1.9377422e-03\n",
      "   8.0774352e-03 -5.9308959e-03  4.5161247e-05 -4.7537349e-03\n",
      "  -9.6035507e-03  5.0072931e-03 -8.7595871e-03 -4.3918253e-03\n",
      "  -3.5099984e-05 -2.9618264e-04 -7.6612402e-03  9.6147414e-03\n",
      "   4.9820566e-03  9.2331432e-03 -8.1579182e-03  4.4957972e-03\n",
      "  -4.1370774e-03  8.2453492e-04  8.4986184e-03 -4.4621779e-03\n",
      "   4.5175003e-03 -6.7869616e-03 -3.5484887e-03  9.3985079e-03\n",
      "  -1.5776539e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      "  -1.5080094e-03  2.4697948e-03 -8.8802812e-04  5.5336617e-03\n",
      "  -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459523e-03\n",
      "  -1.4537406e-03 -9.2081428e-03  4.3705511e-03  5.7178497e-04\n",
      "   7.4419067e-03 -8.1328390e-04 -2.6384138e-03 -8.7530091e-03\n",
      "  -8.5655687e-04  2.8265619e-03  5.4014279e-03  7.0526553e-03\n",
      "  -5.7031228e-03  1.8588186e-03  6.0888622e-03 -4.7980524e-03\n",
      "  -3.1072616e-03  6.7976285e-03  1.6314745e-03  1.8991709e-04\n",
      "   3.4736372e-03  2.1777629e-04  9.6188262e-03  5.0606038e-03\n",
      "  -8.9173913e-03 -7.0415614e-03  9.0145587e-04  6.3925339e-03]\n",
      " [-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419371e-03\n",
      "   7.4669169e-03 -6.1676763e-03  1.1056137e-03  6.0472824e-03\n",
      "  -2.8400517e-03 -6.1735227e-03 -4.1022300e-04 -8.3689503e-03\n",
      "  -5.6000138e-03  7.1045374e-03  3.3525396e-03  7.2256685e-03\n",
      "   6.8002464e-03  7.5307419e-03 -3.7891555e-03 -5.6180713e-04\n",
      "   2.3483753e-03 -4.5190332e-03  8.3887316e-03 -9.8581649e-03\n",
      "   6.7646410e-03  2.9144168e-03 -4.9328329e-03  4.3981862e-03\n",
      "  -1.7395759e-03  6.7113829e-03  9.9648498e-03 -4.3624449e-03\n",
      "  -5.9933902e-04 -5.6956387e-03  3.8508223e-03  2.7866268e-03\n",
      "   6.8910765e-03  6.1010956e-03  9.5384959e-03  9.2734173e-03\n",
      "   7.8980681e-03 -6.9895051e-03 -9.1558648e-03 -3.5575390e-04\n",
      "  -3.0998420e-03  7.8943158e-03  5.9385728e-03 -1.5456629e-03\n",
      "   1.5109634e-03  1.7900396e-03  7.8175711e-03 -9.5101884e-03\n",
      "  -2.0553112e-04  3.4691954e-03 -9.3897345e-04  8.3817719e-03\n",
      "   9.0107825e-03  6.5365052e-03 -7.1162224e-04  7.7104042e-03\n",
      "  -8.5343365e-03  3.2071066e-03 -4.6379971e-03 -5.0889566e-03\n",
      "   3.5896183e-03  5.3703380e-03  7.7695129e-03 -5.7665063e-03\n",
      "   7.4333595e-03  6.6254949e-03 -3.7098003e-03 -8.7456414e-03\n",
      "   5.4374672e-03  6.5097548e-03 -7.8755140e-04 -6.7098569e-03\n",
      "  -7.0859264e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
      "  -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
      "   1.2085581e-03 -2.0748782e-03  2.4402141e-05 -9.8835090e-03\n",
      "   2.6920033e-03 -4.7501065e-03  1.0876465e-03 -1.5762257e-03\n",
      "   2.1966719e-03 -7.8815771e-03 -2.7171851e-03  2.6631975e-03\n",
      "   5.3466819e-03 -2.3915148e-03 -9.5100952e-03  4.5058774e-03]\n",
      " [ 9.4563962e-05  3.0773187e-03 -6.8126465e-03 -1.3754654e-03\n",
      "   7.6685809e-03  7.3464084e-03 -3.6732983e-03  2.6427007e-03\n",
      "  -8.3171297e-03  6.2054847e-03 -4.6373224e-03 -3.1641079e-03\n",
      "   9.3113566e-03  8.7338447e-04  7.4907015e-03 -6.0740639e-03\n",
      "   5.1605059e-03  9.9228211e-03 -8.4573915e-03 -5.1356913e-03\n",
      "  -7.0648384e-03 -4.8626517e-03 -3.7785650e-03 -8.5362010e-03\n",
      "   7.9556061e-03 -4.8439382e-03  8.4236125e-03  5.2625705e-03\n",
      "  -6.5500261e-03  3.9578700e-03  5.4701497e-03 -7.4265362e-03\n",
      "  -7.4057197e-03 -2.4752307e-03 -8.6257271e-03 -1.5815735e-03\n",
      "  -4.0343284e-04  3.2996845e-03  1.4418793e-03 -8.8142155e-04\n",
      "  -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936899e-03\n",
      "   3.9735888e-03  4.5294715e-03  1.4343048e-03 -2.6998566e-03\n",
      "  -4.3668128e-03 -1.0320758e-03  1.4370275e-03 -2.6460099e-03\n",
      "  -7.0737838e-03 -7.8053069e-03 -9.1217877e-03 -5.9351707e-03\n",
      "  -1.8474245e-03 -4.3238713e-03 -6.4606713e-03 -3.7173224e-03\n",
      "   4.2891572e-03 -3.7390448e-03  8.3781742e-03  1.5339922e-03\n",
      "  -7.2423196e-03  9.4337985e-03  7.6312111e-03  5.4932809e-03\n",
      "  -6.8488456e-03  5.8226776e-03  4.0090918e-03  5.1853680e-03\n",
      "   4.2559002e-03  1.9397545e-03 -3.1701636e-03  8.3538434e-03\n",
      "   9.6121784e-03  3.7926030e-03 -2.8369951e-03  7.1263312e-06\n",
      "   1.2188172e-03 -8.4583256e-03 -8.2239462e-03 -2.3101569e-04\n",
      "   1.2372875e-03 -5.7433820e-03 -4.7252751e-03 -7.3460746e-03\n",
      "   8.3286138e-03  1.2129784e-04 -4.5093987e-03  5.7017040e-03\n",
      "   9.1800140e-03 -4.0998720e-03  7.9646800e-03  5.3754328e-03\n",
      "   5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n",
      " [-8.2426788e-03  9.2993546e-03 -1.9766092e-04 -1.9672776e-03\n",
      "   4.6036290e-03 -4.0953159e-03  2.7431131e-03  6.9399667e-03\n",
      "   6.0654259e-03 -7.5107957e-03  9.3823504e-03  4.6718074e-03\n",
      "   3.9661191e-03 -6.2435055e-03  8.4599778e-03 -2.1501661e-03\n",
      "   8.8251876e-03 -5.3620026e-03 -8.1294207e-03  6.8245577e-03\n",
      "   1.6711927e-03 -2.1985101e-03  9.5135998e-03  9.4938539e-03\n",
      "  -9.7740479e-03  2.5052286e-03  6.1566923e-03  3.8724565e-03\n",
      "   2.0227861e-03  4.3050051e-04  6.7363022e-04 -3.8206363e-03\n",
      "  -7.1402504e-03 -2.0888734e-03  3.9238976e-03  8.8186832e-03\n",
      "   9.2591504e-03 -5.9759379e-03 -9.4026709e-03  9.7643761e-03\n",
      "   3.4297847e-03  5.1661157e-03  6.2823440e-03 -2.8042626e-03\n",
      "   7.3227026e-03  2.8302716e-03  2.8710032e-03 -2.3803711e-03\n",
      "  -3.1282497e-03 -2.3701428e-03  4.2764354e-03  7.6057913e-05\n",
      "  -9.5842788e-03 -9.6655441e-03 -6.1481954e-03 -1.2856961e-04\n",
      "   1.9974159e-03  9.4319675e-03  5.5843499e-03 -4.2906976e-03\n",
      "   2.7831554e-04  4.9643586e-03  7.6983096e-03 -1.1442233e-03\n",
      "   4.3234206e-03 -5.8143805e-03 -8.0419064e-04  8.1000496e-03\n",
      "  -2.3600650e-03 -9.6634552e-03  5.7792594e-03 -3.9298222e-03\n",
      "  -1.2228728e-03  9.9805165e-03 -2.2563506e-03 -4.7570658e-03\n",
      "  -5.3293873e-03  6.9808890e-03 -5.7088733e-03  2.1136617e-03\n",
      "  -5.2556610e-03  6.1207130e-03  4.3573068e-03  2.6063537e-03\n",
      "  -1.4910841e-03 -2.7460647e-03  8.9929365e-03  5.2157734e-03\n",
      "  -2.1625208e-03 -9.4703101e-03 -7.4260519e-03 -1.0637427e-03\n",
      "  -7.9494715e-04 -2.5629092e-03  9.6827196e-03 -4.5852186e-04\n",
      "   5.8737611e-03 -7.4475883e-03 -2.5060750e-03 -5.5498648e-03]\n",
      " [-7.1390150e-03  1.2410306e-03 -7.1767163e-03 -2.2446180e-03\n",
      "   3.7193035e-03  5.8331229e-03  1.1981821e-03  2.1027303e-03\n",
      "  -4.1103913e-03  7.2253323e-03 -6.3070417e-03  4.6472144e-03\n",
      "  -8.2199741e-03  2.0364665e-03 -4.9770521e-03 -4.2476892e-03\n",
      "  -3.1089855e-03  5.6552077e-03  5.7983994e-03 -4.9746488e-03\n",
      "   7.7332975e-04 -8.4957788e-03  7.8098057e-03  9.2572905e-03\n",
      "  -2.7423287e-03  8.0022332e-04  7.4665068e-04  5.4778839e-03\n",
      "  -8.6060790e-03  5.8445451e-04  6.8694209e-03  2.2315932e-03\n",
      "   1.1246753e-03 -9.3221571e-03  8.4823659e-03 -6.2641287e-03\n",
      "  -2.9923748e-03  3.4937859e-03 -7.7262876e-04  1.4112901e-03\n",
      "   1.7819905e-03 -6.8288995e-03 -9.7248126e-03  9.0405848e-03\n",
      "   6.1980532e-03 -6.9129276e-03  3.4034825e-03  2.0606279e-04\n",
      "   4.7537447e-03 -7.1199443e-03  4.0269541e-03  4.3474343e-03\n",
      "   9.9573685e-03 -4.4737412e-03 -1.3892651e-03 -7.3173214e-03\n",
      "  -9.6978303e-03 -9.0802573e-03 -1.0227561e-03 -6.5032910e-03\n",
      "   4.8497273e-03 -6.1640264e-03  2.5191857e-03  7.3944090e-04\n",
      "  -3.3921553e-03 -9.7922329e-04  9.9791242e-03  9.1458866e-03\n",
      "  -4.4618296e-03  9.0830252e-03 -5.6417631e-03  5.9309220e-03\n",
      "  -3.0972194e-03  3.4317516e-03  3.0172253e-03  6.9004609e-03\n",
      "  -2.3738837e-03  8.7750368e-03  7.5894282e-03 -9.5476462e-03\n",
      "  -8.0082109e-03 -7.6378966e-03  2.9232574e-03 -2.7947235e-03\n",
      "  -6.9295214e-03 -8.1282640e-03  8.3091781e-03  1.9904876e-03\n",
      "  -9.3280170e-03 -4.7927164e-03  3.1367373e-03 -4.7132061e-03\n",
      "   5.2808430e-03 -4.2334413e-03  2.6417947e-03 -8.0456873e-03\n",
      "   6.2098862e-03  4.8188879e-03  7.8719138e-04  3.0134462e-03]\n",
      " [-8.7274835e-03  2.1301603e-03 -8.7354420e-04 -9.3190884e-03\n",
      "  -9.4281435e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
      "  -6.4986944e-03 -6.8730689e-03 -4.9994136e-03 -2.2868442e-03\n",
      "  -7.2502876e-03 -9.6033188e-03 -2.7436304e-03 -8.3628418e-03\n",
      "  -6.0388758e-03 -5.6709289e-03 -2.3441387e-03 -1.7069983e-03\n",
      "  -8.9569995e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
      "  -7.2061159e-03 -3.6668323e-03  3.1185509e-03 -9.5707225e-03\n",
      "   1.4764380e-03  6.5244650e-03  5.7464195e-03 -8.7630628e-03\n",
      "  -4.5171450e-03 -8.1401607e-03  4.5955181e-05  9.2636319e-03\n",
      "   5.9733056e-03  5.0673080e-03  5.0610616e-03 -3.2429171e-03\n",
      "   9.5521836e-03 -7.3564244e-03 -7.2703888e-03 -2.2653891e-03\n",
      "  -7.7856064e-04 -3.2161046e-03 -5.9258699e-04  7.4888230e-03\n",
      "  -6.9751980e-04 -1.6249418e-03  2.7443981e-03 -8.3591007e-03\n",
      "   7.8558037e-03  8.5361032e-03 -9.5840879e-03  2.4462652e-03\n",
      "   9.9049713e-03 -7.6658037e-03 -6.9669201e-03 -7.7365185e-03\n",
      "   8.3959224e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
      "   3.7430834e-03  2.6350426e-03  7.4271200e-04  2.3276759e-03\n",
      "  -7.4690939e-03 -9.3583753e-03  2.3545765e-03  6.1484552e-03\n",
      "   7.9856869e-03  5.7358933e-03 -7.7733753e-04  8.3061643e-03\n",
      "  -9.3363142e-03  3.4061312e-03  2.6675223e-04  3.8572431e-03\n",
      "   7.3857834e-03 -6.7251683e-03  5.5844807e-03 -9.5222257e-03\n",
      "  -8.0446003e-04 -8.6887386e-03 -5.0986744e-03  9.2892265e-03\n",
      "  -1.8582630e-03  2.9144264e-03  9.0712784e-03  8.9381309e-03\n",
      "  -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044296e-03\n",
      "  -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758990e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Access the 100D vectors for all 6 words\n",
    "\n",
    "print(model.wv.__getitem__(model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 100)\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.__getitem__(model.wv.index_to_key).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=6, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model.save('model/first_word_vectors.bin')\n",
    "\n",
    "# load model\n",
    "new_model = Word2Vec.load('model/first_word_vectors.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = new_model.wv.__getitem__(new_model.wv.index_to_key)\n",
    "pca = PCA(n_components = 2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "words = list(new_model.wv.index_to_key)\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13887984"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('best', 'worst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', 0.17018884420394897),\n",
       " ('best', 0.06408978998661041),\n",
       " ('wisdom', -0.013514966703951359),\n",
       " ('time', -0.023671671748161316),\n",
       " ('age', -0.05234675854444504)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('foolish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec vs BERT\n",
    "\n",
    "**Embeddings**  \n",
    "Word2Vec offers pre-trained word embeddings that anyone can use off-the-shelf. The embeddings are key: value pairs, essentially 1-1 mappings between words and their respective vectors. Word2Vec takes a single word as input and outputs a single vector representation of that word. \n",
    "\n",
    "Since BERT generates contextual embeddings, it takes as input a sequence (usually a sentence) rather than a single word. BERT needs to be shown the context that surrounding words provide before it can generate a word embedding. With BERT, you do need to have the actual model as the vector representations of words will vary based on the specific sequences you’re inputting. The output is a fixed-length vector representation of the input sentence. \n",
    "\n",
    "BERT or Bidirectional Encoder Representations from Transformers, is a technique that allows for bidirectional training of Transformers for natural language modeling tasks. Language models which are bidirectionally trained can learn deeper context from language than single-direction models. BERT generates context aware embeddings that allow for multiple representations (each representation, in this case, is a vector) of each word based on a given word’s context.\n",
    "\n",
    "**Word Ordering**  \n",
    "Word2Vec embeddings do not take into account the word position.\n",
    "\n",
    "BERT model explicitly takes as input the position (index) of each word in the sentence before calculating its embedding.\n",
    "\n",
    "**Out-of-Vocabulary**  \n",
    "Since Word2Vec learns embeddings at word level, it can only generate embeddings for words that existed in it’s training set (aka it’s “vocabulary space”). This is a major drawback to Word2Vec - that it just doesn’t support Out-of-Vocabulary words.\n",
    "\n",
    "Alternatively, BERT learns representations at the subword level, so a BERT model will have a smaller vocabulary space than the number of unique words in its training corpus. In turn, BERT is able to generate embeddings for words outside of its vocabulary space giving it a near infinite vocabulary. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
